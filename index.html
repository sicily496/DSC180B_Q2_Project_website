<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/pipeline.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="static/js/pipeline.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Balancing Accuracy and Efficiency: A Comparative Study
              of Knowledge Distillation and Post-Training Quantization
              Sequences</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="mailto:dgitelman@ucsd.edu" target="_blank">Daniel Gitelman</a>,</span>
                <span class="author-block">
                  <a href="mailto:zhw055@ucsd.edu" target="_blank">Jiangqi Wu</a>,</span>
                  <span class="author-block">
                    <a href="mailto:zhw055@ucsd.edu" target="_blank">Vishwak Pabba</a>,</span>
                    <span class="author-block">
                      <a href="mailto:zhw055@ucsd.edu" target="_blank">Zhiqing Wang</a></a>
                  </span>
              <br>
              <span class="author-block">
                <a href="mailto:zhw055@ucsd.edu" target="_blank">Alex Cloninger</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="mailto:zhw055@ucsd.edu" target="_blank">Rayan Saab</a><sup>*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">UC San Diego - Halıcıoğlu Data Science Institute (HDSI)</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Mentor</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!--  PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/poster.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Poster</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/jiangqiw/DSC180B_Q2_Project/tree/main" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper introduction -->
<section class="section hero is-light">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Deep neural networks (DNNs) have become the cornerstone of modern machine
learning, but often demand extensive computational resources. To address this gap,
model compression techniques have emerged as a vital area of research, aiming to reduce computational and storage overhead while preserving accuracy. Existing methods, such as knowledge distillation (transferring knowledge from a large ”teacher” model to a compact ”student”
model) and quantization (reducing numerical precision of weights), have shown promise individually. However, the interplay between these techniques—particularly their combined impact on accuracy, storage efficiency, and inference speed—remains underexplored. In this project, we propose a hybrid compression
strategy that systematically integrates knowledge distillation and post-training quantization to optimize the trade-off between model efficiency and performance.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper Dataset -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3"> Dataset </h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/CIFAR-10.png" alt="cifar10 sample image"/>
        <h2 class="subtitle has-text-centered">
          CIFAR-10: 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/CIFAR-100.png" alt="cifar100 sample image"/>
        <h2 class="subtitle has-text-centered">
          CIFAR-100: 100 (fine) classes (belong to 20 Superclasses, eg. aquatic mammals: beaver, dolphin, otter, seal, whale)
        </h2>
      </div>
  </div>
  <ul>
    <li><strong>Image Size</strong>: 32×32 pixels</li>
    <li><strong>Number of Images</strong>: 60,000 (50,000 Training Images + 10,000 Test Images)
    </li>
</ul>
<p><strong>Reference</strong>: <a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank">Alex Krizhevsky, "Learning multiple layers of features from tiny images, 2009."</a></p>
    </div>
  </div>
</div>
</div>
</section>
<!-- End dataset -->

<!-- Paper question -->
<section class="section hero is-light">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Research Question</h2>
        <div class="content has-text-justified">
          <p>The primary inquiry of this research is to determine if a hybrid compression strategy, integrating Knowledge Distillation with Quantization, outperforms a singular approach.</p>
          <p style="color: #3273dc;"><strong>Specific Question: Does quantizing a student model, derived through Knowledge Distillation, achieve greater efficiency than quantizing the teacher model?</strong></p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End question -->

<!-- Paper method -->
<section class="hero is-small is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Pipeline</h2>
          <div class="pipeline-container">
            <!-- First Vertical Column (Main Pipeline) -->
            <div class="pipeline-sections">
                <!-- Section 1 -->
                <div class="pipeline-section" id="section1" data-text="Number of layers:50
                Number of parameter: 25557032
                Bit size: 32
                File size: 97.8 MB
                Accuracy: Higher accuracy
                Computational resources: More demanding">
                    Resnet 50<br>Teacher's Model
                </div>
                <!-- Arrow 1 (Down) -->
                <div class="arrow-container" id="arrow1">
                    <span class="arrow-text">KD Strategies</span>
                    <div class="arrow">↓</div>
                </div>
        
                <!-- Section 2 -->
                <div class="pipeline-section" id="section2" data-text="Number of layers:18
                Number of parameter: 11689512
                Bit size: 32
                File size: 44.7 MB
                Accuracy: Lower accuracy
                Computational resources: More efficient">
                    Resnet 18<br>Student's Model
                </div>
        
                <!-- Arrow 2 (Down) -->
                <div class="arrow-container" id="arrow2">
                    <span class="arrow-text">Post-Training<br>Quantization</span>
                    <div class="arrow">↓</div>
                </div>
        
                <!-- Section 3 -->
                <div class="pipeline-section" id="section3" data-text="Number of layers:18
                Number of parameter: 11689512
                Bit size: <32 (depend on requirement)
                File size: <44.7 MB (depend on requirement)
                Accuracy: Lower accuracy
                Computational resources: More efficient">
                    Quantized Resnet 18<br>Student Model
                </div>
            </div>
        
            <!-- Second Vertical Column (Section 4 aligned with Section 3) -->
            <div class="para-container">
                <div class="para-section" id="section4" data-text="Number of layers:50
                Number of parameter: 25557032
                Bit size: <32 (depend on requirement)
                File size: <97.8 MB (depend on requirement)
                Accuracy: Lower accuracy
                Computational resources: More efficient">
                  Quantized Resnet 50<br>Teacher Model
                </div>
            </div>
            <!-- L-Shaped Arrow -->
            <div class="arrow-container l-shape" id="arrow3">
              <span class="arrow down">↓</span>
              <span class="arrow-text-middle">Post-Training Quantization</span>
          </div>
          
            <!-- Third Vertical Column (Hover Text) -->
            <div class="hover-text">              
              Number of layers:50\n
              Number of parameter: 25557032\n
              Bit size: 32\n
              File size: 97.8 MB\n
              Accuracy: Higher accuracy\n
              Computational resources: More demanding\n
            </div>
        </div>
    </div>
  </div>
</section>
<!-- End method -->


<!-- Paper method -->
<section class="section hero is-light">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
          <p>Initially, we assess various KD techniques, then apply a greedy path-following quantization algorithm to reduce the student model's bit size. 
            The subsequent sections detail the processes of knowledge distillation and quantization separately.
          </p>
          <h4>KD Strategies</h4>
          <ul>
            <li>
              <strong>Vanilla Knowledge Distillation (VKD):</strong>
              <p>This strategy employs a hybrid loss combining cross-entropy from ground-truth labels (<i>L<sub>C</sub></i>) and Kullback-Leibler divergence from the teacher’s soft targets (<i>D<sub>KL</sub></i>), refined with temperature scaling <i>T</i>. The loss formula is:</p>
              <ul>
                <li>L = (1 - &alpha;) &middot; L<sub>C</sub>(y, <span style="position: relative; display: inline-block;"><span style="position: absolute; left: 50%; transform: translateX(-50%) translateY(-0.2em);">&circ;</span>y</span>) + &alpha; &middot; T<sup>2</sup> &middot; D<sub>KL</sub>(p, q)</p></li> 
              </ul> 
            </li>      
            <li>
                <strong>"Mixup" Method for Data Generation (Mixup):</strong>
                <p>This strategy improves knowledge distillation performance by creating new training samples through the linear interpolation of image and label pairs. Given two samples <i>(x<sub>i</sub>, y<sub>i</sub>)</i> and <i>(x<sub>j</sub>, y<sub>j</sub>)</i>, it generates:</p>
                <ul>
                  <li><span style="position: relative; display: inline-block;"><span style="position: absolute; left: 0%; transform: translateY(-0.4em);">~</span><i>x</i></span> = &lambda;x<sub>i</sub> + (1 - &lambda;)x<sub>j</sub></li>
                  <li><span style="position: relative; display: inline-block;"><span style="position: absolute; left: 0%; transform: translateY(-0.4em);">~</span><i>y</i></span> = &lambda;y<sub>i</sub> + (1 - &lambda;)y<sub>j</sub></li>
                </ul>                  
              </li>
              <li>
              <p><strong>Deep Mutual Learning (DML):</strong> 
                <p>This strategy utilizes two ResNet18 student models with different initial weights: one pretrained on CIFAR-10/CIFAR-100 and another with ImageNet-derived weights. This approach promoted diverse learning outcomes and speed up knowledge acquisition, particularly reducing training time for the CIFAR-pretrained model.</p>
                <p>During training, we optimized both models, <i>&Theta;<sub>1</sub></i> and <i>&Theta;<sub>2</sub></i>, with respective loss functions were:</p>                
              <ul>
                <li><i>L<sub>&Theta;<sub>1</sub></sub></i> = L<sub>C<sub>1</sub></sub> + D<sub>KL</sub>(p<sub>2</sub> &parallel; p<sub>1</sub>)</li>
                <li><i>L<sub>&Theta;<sub>2</sub></sub></i> = L<sub>C<sub>2</sub></sub> + D<sub>KL</sub>(p<sub>1</sub> &parallel; p<sub>2</sub>)</li>
              </ul>
              </li>
              <li>
                  <strong>Decoupled Knowledge Distillation (DKD):</strong>
                  <p>This strategy divides the distillation loss into two key components:</p>
                  <ul>
                    <li>Target Class Knowledge Distillation (TCKD): Focuses on aligning the student's predicted probability for the correct class with that of the teacher, ensuring accurate classification learning.</li>
                    <li>Non-Target Class Knowledge Distillation (NCKD): Aims to match the student's predicted probabilities for incorrect classes to the teacher's, enhancing overall probability distribution accuracy.</li>
                  </ul>
                  <p></p>The loss functions for these components are optimized using parameters &alpha; and &beta;, represented by the formula:
                  <ul>
                    <li>L = &alpha; &middot; L<sub>TCKD</sub>(p<sub>correct</sub>, q<sub>correct</sub>) + &beta; &middot; L<sub>NCKD</sub>(p<sub>incorrect</sub>, q<sub>incorrect</sub>)</li>
                  </ul>
              </li>
          </ul>
          <h4>Post-Training Quantization</h4>
          <ul>
              <li>
                  <strong>GPFQ (Greedy Post-Training Feature Quantization):</strong>
                  <p>This strategy utilizes greedy layer-wise quantization guided by accuracy impact analysis, compressing model while maintaining performance.</p>
              </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End pipeline -->


<!-- Paper result -->
<section class="hero is-small is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Experiment Results</h2>
          <div class="content has-text-justified">
          <p> Our experimental findings present the performance of quantized student models, employing various knowledge distillation techniques, and a quantized teacher model across a range of bit sizes (2, 3, 4, 5, 6, 7, 8, 12, 16, 20, 26, 32).</p>
            <style>
              table {
                  width: 100%;
                  border-collapse: collapse;
                  margin: 20px 0;
                  font-family: Arial, sans-serif;
              }
              th, td {
                  padding: 8px;
                  text-align: center;
                  border-bottom: 1px solid #ddd;
              }
              th {
                  background-color: #f2f2f2;
              }
              tr:hover {background-color: #f5f5f5;}
          </style>
            <h3>CIFAR-10</h3>
            <table>
                <tr>
                    <th>Model</th>
                    <th>2-bit</th>
                    <th>3-bit</th>
                    <th>4-bit</th>
                    <th>5-bit</th>
                    <th>6-bit</th>
                    <th>7-bit</th>
                    <th>8-bit</th>
                    <th>32-bit</th>
                </tr>
                <tr>
                    <td>Teacher</td>
                    <td>90.90</td>
                    <td>91.29</td>
                    <td>91.55</td>
                    <td>91.60</td>
                    <td>91.88</td>
                    <td>91.88</td>
                    <td>92.00</td>
                    <td>92.25</td>
                </tr>
                <tr>
                    <td>VKD Student</td>
                    <td>71.87</td>
                    <td>82.40</td>
                    <td>86.47</td>
                    <td>88.78</td>
                    <td>88.89</td>
                    <td>89.36</td>
                    <td>89.69</td>
                    <td>90.77</td>
                </tr>
                <tr>
                    <td>Mixup Student</td>
                    <td>88.12</td>
                    <td>92.63</td>
                    <td>94.20</td>
                    <td>95.09</td>
                    <td>95.21</td>
                    <td>95.18</td>
                    <td>95.45</td>
                    <td>95.77</td>
                </tr>
                <tr>
                    <td>DML Student</td>
                    <td>82.39</td>
                    <td>86.98</td>
                    <td>90.45</td>
                    <td>91.84</td>
                    <td>92.00</td>
                    <td>92.43</td>
                    <td>92.54</td>
                    <td>92.89</td>
                </tr>
                <tr>
                    <td>DKD Student</td>
                    <td>61.06</td>
                    <td>69.69</td>
                    <td>77.69</td>
                    <td>81.76</td>
                    <td>83.45</td>
                    <td>84.16</td>
                    <td>84.39</td>
                    <td>89.95</td>
                </tr>
            </table> 
            <h3>CIFAR-100</h3> 
            <table>
              <tr>
                  <th>Model</th>
                  <th>2-bit</th>
                  <th>3-bit</th>
                  <th>4-bit</th>
                  <th>5-bit</th>
                  <th>6-bit</th>
                  <th>7-bit</th>
                  <th>8-bit</th>
                  <th>32-bit</th>
              </tr>
              <tr>
                  <td>Teacher</td>
                  <td>66.63</td>
                  <td>74.17</td>
                  <td>75.06</td>
                  <td>75.84</td>
                  <td>75.73</td>
                  <td>75.78</td>
                  <td>75.80</td>
                  <td>76.43</td>
              </tr>
              <tr>
                  <td>VKD Student</td>
                  <td>26.23</td>
                  <td>47.65</td>
                  <td>59.75</td>
                  <td>65.93</td>
                  <td>68.70</td>
                  <td>69.85</td>
                  <td>70.54</td>
                  <td>75.33</td>
              </tr>
              <tr>
                  <td>Mixup Student</td>
                  <td>33.07</td>
                  <td>46.50</td>
                  <td>57.10</td>
                  <td>60.33</td>
                  <td>61.83</td>
                  <td>61.77</td>
                  <td>62.16</td>
                  <td>62.17</td>
              </tr>
              <tr>
                  <td>DML Student</td>
                  <td>19.22</td>
                  <td>37.06</td>
                  <td>52.96</td>
                  <td>62.94</td>
                  <td>65.98</td>
                  <td>69.89</td>
                  <td>70.87</td>
                  <td>75.12</td>
              </tr>
              <tr>
                  <td>DKD Student</td>
                  <td>22.20</td>
                  <td>33.11</td>
                  <td>44.78</td>
                  <td>49.76</td>
                  <td>52.20</td>
                  <td>52.76</td>
                  <td>52.85</td>
                  <td>58.01</td>
              </tr>
          </table>        
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <!-- Adjust the image size using inline CSS -->
                <img src="static/images/fig1-100.png" alt="cifar100 result" style="max-width: 70%; height: auto; display: block; margin: 0 auto;">
              </div>
              <div class="item">
                <!-- Adjust the image size using inline CSS -->
                <img src="static/images/fig2-100.png" alt="cifar100 result" style="max-width: 70%; height: auto; display: block; margin: 0 auto;">
              </div>
            </div>            
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End result -->

<!-- Paper result -->
<section class="section hero is-light">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
          <div class="content has-text-justified">
              <strong>CIFAR-100:</strong>
              <ul>
                  <li>At 2–6 bits, all students initially drop in accuracy more sharply than the teacher.</li>
                  <li>Distillation aids complex datasets but reduces compressibility for further quantization.</li>
              </ul>
          
              <strong>CIFAR-10:</strong>
              <ul>
                  <li>At 2–4 bits, all students see a sharper accuracy drop.</li>
                  <li>Both models are robust at higher bit widths, with distillation helping the student nearly match its pre-quantization accuracy.</li>
              </ul>
          
              <p>Overall, the quantized teacher model outperforms the quantized student model, particularly at small bit sizes, such as 2 bits. However, this advantage diminishes when applied to less complex datasets.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End result -->

<section class="section" id="BibTeX">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Reference</h2>
        <div class="content has-text-justified">
        <div class="references">
          <ol>
              <li>
                  Cristian Bucilǎ, Rich Caruana, and Alexandru Niculescu-Mizil, "Model compression," in <em>Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, ACM, New York, NY, USA, 2006, pp. 535–541. <a href="https://doi.org/10.1145/1150402.1150464">https://doi.org/10.1145/1150402.1150464</a>
              </li>
              <li>
                  Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean, "Distilling the Knowledge in a Neural Network," <em>CoRR</em>, vol. abs/1503.02531, 2015. <a href="http://arxiv.org/abs/1503.02531">http://arxiv.org/abs/1503.02531</a>
              </li>
              <li>
                  Adriana Romero et al., "FitNets: Hints for Thin Deep Nets," <em>3rd International Conference on Learning Representations, ICLR 2015</em>, San Diego, CA, USA, 2015. <a href="http://arxiv.org/abs/1412.6550">http://arxiv.org/abs/1412.6550</a>
              </li>
              <li>
                  Wonpyo Park et al., "Relational Knowledge Distillation," 2019. <a href="https://arxiv.org/abs/1904.05068">https://arxiv.org/abs/1904.05068</a>
              </li>
              <li>
                  Ahmed T. Elthakeb et al., "Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks," 2020. <a href="https://arxiv.org/abs/1906.06033">https://arxiv.org/abs/1906.06033</a>
              </li>
              <li>
                  Lingyu Gu et al., "\"Lossless\" Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach," 2024. <a href="https://arxiv.org/abs/2403.00258">https://arxiv.org/abs/2403.00258</a>
              </li>
              <li>
                  Jinjie Zhang et al., "Post-training Quantization for Neural Networks with Provable Guarantees," 2023. <a href="https://arxiv.org/abs/2201.11113">https://arxiv.org/abs/2201.11113</a>
              </li>
              <li>
                  Ying Zhang et al., "Deep Mutual Learning," 2017. <a href="https://arxiv.org/abs/1706.00384">https://arxiv.org/abs/1706.00384</a>
              </li>
              <li>
                  Lucas Beyer et al., "Knowledge distillation: A good teacher is patient and consistent," 2022. <a href="https://arxiv.org/abs/2106.05237">https://arxiv.org/abs/2106.05237</a>
              </li>
              <li>
                  Borui Zhao et al., "Decoupled Knowledge Distillation," 2022. <a href="https://arxiv.org/abs/2203.08679">https://arxiv.org/abs/2203.08679</a>
              </li>
          </ol>
        </div>    
      </div>
    </div>
  </div>
</div>
</section>

<!-- Statcounter tracking code -->
<!-- End of Statcounter Code -->

  </body>
  </html>
